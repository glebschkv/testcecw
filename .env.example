# ============================================
# OBD InsightBot Configuration
# ============================================

# --------------------------------------------
# LOCAL GRANITE MODEL (Recommended - No Server Needed)
# --------------------------------------------
# Uses llama-cpp-python to run Granite models directly.
# No Ollama or external server required!
# The model auto-downloads from HuggingFace on first run.

# HuggingFace repo and file for auto-download
GRANITE_MODEL_REPO=ibm-granite/granite-3.3-2b-instruct-GGUF
GRANITE_MODEL_FILE=granite-3.3-2b-instruct.Q4_K_M.gguf

# Or set a direct path to a pre-downloaded GGUF file
# GRANITE_MODEL_PATH=/path/to/your/model.gguf

# Model settings
GRANITE_N_CTX=2048
GRANITE_N_GPU_LAYERS=0

# Available Granite GGUF models:
# - granite-3.3-2b-instruct.Q4_K_M.gguf  (lighter, ~1.5GB, faster)
# - granite-3.3-8b-instruct.Q4_K_M.gguf  (more capable, ~5GB)

# --------------------------------------------
# IBM watsonx.ai (Cloud - Optional)
# --------------------------------------------
# Only needed if local model is not available
# Get credentials at: https://cloud.ibm.com/watsonx

WATSONX_URL=https://us-south.ml.cloud.ibm.com
WATSONX_API_KEY=your_api_key_here
WATSONX_PROJECT_ID=your_project_id_here

# --------------------------------------------
# IBM Watson Speech Services (Optional)
# --------------------------------------------
# Only needed for voice features (BR6/BR7)

WATSON_SPEECH_API_KEY=your_speech_api_key_here
WATSON_SPEECH_URL=https://api.us-south.speech-to-text.watson.cloud.ibm.com

# --------------------------------------------
# Application Settings
# --------------------------------------------
APP_DEBUG=false
APP_LOG_LEVEL=INFO
DATABASE_PATH=./data/obd_insightbot.db
